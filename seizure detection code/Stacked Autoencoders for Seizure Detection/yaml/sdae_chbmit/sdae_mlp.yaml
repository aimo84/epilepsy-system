!obj:pylearn2.train.Train {
    # Dataset for training
    dataset: &train !obj:pylearn2ext.chbmit.CHBMIT {
        patient_id: %(patient_id)i,
        which_set: 'train',
        preprocessor_path: '%(save_path)s/sdae_scaler.pkl',
        data_dir: '%(data_path)s',
        transform: 'single_channel',
        leave_one_out_file: %(leave_one_out_file)i,
        window_size: %(window_size)i,
        batch_size: %(batch_size)i
    },
    model: !obj:pylearn2.models.mlp.MLP {
        batch_size: %(batch_size)i,
        layers: [
                 !obj:pylearn2ext.mlp.PretrainedLayerWeight {
                     layer_name: 'ae1',
                     layer_content: !pkl: "%(save_path)s/sdae_l1.pkl"
                 },
                 !obj:pylearn2ext.mlp.PretrainedLayerWeight {
                     layer_name: 'ae2',
                     layer_content: !pkl: "%(save_path)s/sdae_l2.pkl"
                 },
                 !obj:pylearn2.models.mlp.Softmax {
                     layer_name: 'y',
                     n_classes: %(n_classes)i,
                     irange: .01
                 }
                ],
        nvis: %(window_size)i
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        learning_rate: .05,
#        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
#            init_momentum: .5
#        },
        batch_size : %(batch_size)i,
        monitoring_batches : %(monitoring_batches)i,
        monitoring_dataset: {
            'train' : *train,
            'valid' : !obj:pylearn2ext.chbmit.CHBMIT {
                patient_id: %(patient_id)i,
                which_set: 'valid',
                preprocessor_path: '%(save_path)s/sdae_scaler.pkl',
                data_dir: '%(data_path)s',
                transform: 'single_channel',
                leave_one_out_file: %(leave_one_out_file)i,
                window_size: %(window_size)i,
                batch_size: %(batch_size)i
            }
        },
        cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.mlp.Default {},
                !obj:pylearn2.costs.mlp.WeightDecay {
                    coeffs: [
                        3e-3,
                        3e-3,
                        3e-3
                    ]
                }
            ]
        },
        termination_criterion: !obj:pylearn2.termination_criteria.And {
            criteria: [
#                !obj:pylearn2.termination_criteria.MonitorBased {
#                    channel_name: "valid_y_misclass",
#                    prop_decrease: 0.,
#                    N: 100
#                },
                !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: %(max_epochs)i
                }
            ]
        }
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
            channel_name: 'valid_y_misclass',
            save_path: "%(save_path)s/sdae_all_best.pkl"
        },
        # Adjust the learning rate of the SGD algorithm at the end of each epoch
        # It starts decreasing the learning rate after 1 epoch , and to continue increasing it until
        #  it reaches a value of (learning rate * 0.2) at the end of the 'saturate' epoch.
        !obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {
            start: 20,
            saturate: 30,
            decay_factor: 0.2
        }
#        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {
#            start: 1,
#            saturate: 20,
#            final_momentum: .9
#        }
    ],

    save_path: "%(save_path)s/sdae_all.pkl",
    save_freq: 2
}
